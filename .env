# Example env for ESPDragon CLI
# Replace values with your deployment's configuration

# Base public URL for your Ollama server (required for LLM)
# Use http://localhost:11434 for local development
# For production, use the public Codespaces URL
OLLAMA_PUBLIC_URL="http://localhost:11434"
# Model to use for the Ollama run (requested)
# Available models: qwen2.5:3b (currently installed)
# To use other models, install them with: ollama pull <model-name>
OLLAMA_MODEL="qwen2.5:3b"
OLLAMA_API_PATH="/api/generate"
# Optional API key for Ollama if required
# OLLAMA_API_KEY="..."

# Confirm scanning authorization via env (alternative to --confirm flag)
# WARNING: Setting this asserts you have explicit authorization to scan targets.
ESPDRAGON_AUTHORIZED="yes"

# Optional runtime tuning
# OLLAMA_TEMPERATURE="0.2"
# OLLAMA_MAX_TOKENS="512"
